\documentclass[10pt]{beamer}
\usepackage{tabulary}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{tabulary}
\usepackage{caption} 
\usepackage{listings}
\usecolortheme{whale}
\usepackage{algorithm}
\usepackage{algorithm2e}
\usepackage{algpseudocode}
\usepackage[ruled,vlined,noend]{algorithm2e}

\usetheme{metropolis}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]

\title{\textsc{Newton Raphson Method}}
\date{\today}
\author{Swapnil Singh \\ Bank of Lithuania, KTU}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}[fragile]{Outline}
    \begin{itemize}
        \item Introduction to Newton-Raphson Method
        \item Mathematical Foundations
        \item Line Search and Armijo Rule
        \item Convergence Properties
        \item Algorithm Implementation
        \item Practical Considerations
        \item Examples and Applications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Introduction}
    \begin{itemize}
        \item \textbf{Purpose:} Find roots of equation $f(x) = 0$ or minimum/maximum of function
        \item \textbf{Key Idea:} Use local quadratic approximation
        \item \textbf{Advantages:}
        \begin{itemize}
            \item Quadratic convergence when close to solution
            \item Efficient for smooth functions
            \item Works well for multi-dimensional problems
        \end{itemize}
        \item \textbf{Disadvantages:}
        \begin{itemize}
            \item Requires derivatives
            \item Sensitive to starting point
            \item May not converge for poor initial guesses
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Mathematical Foundation}
    \textbf{For Root Finding:}
    \begin{align*}
        x_{k+1} &= x_k - \frac{f(x_k)}{f'(x_k)}
    \end{align*}
    
    \textbf{For Optimization:}
    \begin{itemize}
        \item Find where gradient is zero: $\nabla f(x) = 0$
        \item Update formula with line search:
        \begin{align*}
            x_{k+1} &= x_k + \alpha_k d_k \\
            d_k &= -[H(x_k)]^{-1}\nabla f(x_k)
        \end{align*}
        where:
        \begin{itemize}
            \item $\nabla f(x_k)$ is the gradient
            \item $H(x_k)$ is the Hessian matrix
            \item $\alpha_k$ is the step size from line search
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Quadratic Approximation}
    \textbf{Taylor Series Expansion:}
    \begin{align*}
        f(x_k + d) &\approx f(x_k) + \nabla f(x_k)^T d + \frac{1}{2}d^T H(x_k)d
    \end{align*}
    
    \textbf{Finding the Minimum:}
    \begin{itemize}
        \item Take derivative with respect to $d$ and set to zero:
        \[ \nabla f(x_k) + H(x_k)d = 0 \]
        \item Solve for the Newton direction:
        \[ d_k = -H(x_k)^{-1}\nabla f(x_k) \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Line Search with Armijo Rule}
    \textbf{Armijo Condition:}
    \begin{itemize}
        \item Ensures sufficient decrease in function value
        \item Step size $\alpha_k$ must satisfy:
        \[ f(x_k + \alpha_k d_k) \leq f(x_k) + c\alpha_k\nabla f(x_k)^T d_k \]
        where:
        \begin{itemize}
            \item $c$ is typically 0.1 (Armijo parameter)
            \item $\alpha_k$ is reduced by factor $\beta$ (typically 0.5)
        \end{itemize}
    \end{itemize}
    
    \textbf{Benefits:}
    \begin{itemize}
        \item Guarantees descent property
        \item Improves global convergence
        \item Handles poor quadratic approximations
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Convergence Properties}
    \textbf{Local Convergence:}
    \begin{itemize}
        \item Quadratic convergence near solution:
        \[ \|e_{k+1}\| \leq C\|e_k\|^2 \]
        \item Requires:
        \begin{itemize}
            \item Smooth function
            \item Good initial guess
            \item Non-singular Hessian at solution
        \end{itemize}
    \end{itemize}
    
    \textbf{Global Convergence with Line Search:}
    \begin{itemize}
        \item Armijo rule ensures:
        \[ f(x_{k+1}) < f(x_k) \]
        \item Convergence to local minimum under mild conditions
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Algorithm Implementation}
    \begin{algorithm2e}
    \SetAlgoLined
    \KwIn{Initial guess $x_0$, tolerance $\epsilon$, max iterations $N$}
    \KwIn{Armijo parameter $c$, reduction factor $\beta$}
    \KwOut{Optimal point $x^*$}
    
    $k \leftarrow 0$\;
    \While{$k < N$}{
        Compute gradient: $g_k = \nabla f(x_k)$\;
        Compute Hessian: $H_k = H(x_k)$\;
        \If{$\|g_k\| < \epsilon$}{
            \Return{$x_k$}\;
        }
        Solve: $H_k d_k = -g_k$\;
        $\alpha_k \leftarrow 1$\;
        \While{$f(x_k + \alpha_k d_k) > f(x_k) + c\alpha_k g_k^T d_k$}{
            $\alpha_k \leftarrow \beta\alpha_k$\;
            \If{$\alpha_k < 10^{-10}$}{
                break\;
            }
        }
        Update: $x_{k+1} = x_k + \alpha_k d_k$\;
        $k \leftarrow k + 1$\;
    }
    \Return{$x_k$}
    \caption{Newton-Raphson Optimization with Armijo Line Search}
    \end{algorithm2e}
\end{frame}

\begin{frame}[fragile]{Implementation Details}
    \textbf{Key Components:}
    \begin{itemize}
        \item \textbf{Function Evaluation:}
        \begin{itemize}
            \item Define objective function
            \item Compute gradient analytically or numerically
            \item Compute Hessian analytically or numerically
        \end{itemize}
        \item \textbf{Linear System Solution:}
        \begin{itemize}
            \item Use \texttt{mldivide} (backslash) operator
            \item Check condition number
            \item Handle singular matrices
        \end{itemize}
        \item \textbf{Line Search:}
        \begin{itemize}
            \item Implement backtracking
            \item Choose appropriate parameters
            \item Handle convergence failures
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Practical Considerations}
    \textbf{Implementation Challenges:}
    \begin{itemize}
        \item Choose appropriate stopping criteria
        \item Handle ill-conditioned Hessians
        \item Select good initial point
        \item Deal with non-convex functions
        \item Choose appropriate line search parameters
    \end{itemize}
    
    \textbf{Line Search Parameters:}
    \begin{itemize}
        \item Armijo parameter $c$ (typically 0.1)
        \item Step reduction factor $\beta$ (typically 0.5)
        \item Minimum step size threshold
    \end{itemize}
    
    \textbf{Improvements:}
    \begin{itemize}
        \item Strong Wolfe conditions
        \item Trust region methods
        \item Quasi-Newton methods (BFGS)
        \item Regularization for ill-conditioning
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example Applications}
    \textbf{Optimization Problems:}
    \begin{itemize}
        \item Maximum likelihood estimation
        \item Least squares problems
        \item Portfolio optimization
        \item Machine learning (model training)
    \end{itemize}
    
    \textbf{Root Finding:}
    \begin{itemize}
        \item Solving nonlinear equations
        \item Finding equilibrium points
        \item Solving boundary value problems
        \item Financial derivatives pricing
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Summary}
    \textbf{Key Points:}
    \begin{itemize}
        \item Newton-Raphson combines quadratic approximation with line search
        \item Armijo rule ensures sufficient decrease
        \item Quadratic convergence near solution
        \item Practical implementation requires careful attention to details
    \end{itemize}
    
    \textbf{When to Use:}
    \begin{itemize}
        \item Smooth, well-behaved functions
        \item When derivatives are available
        \item When fast local convergence is needed
        \item When good initial guess is available
    \end{itemize}
\end{frame}

\end{document}